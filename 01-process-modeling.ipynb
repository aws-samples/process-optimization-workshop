{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60b13f9",
   "metadata": {},
   "source": [
    "# 01 - Continuous stirred tank reactor (CSTR)\n",
    "\n",
    "**Amazon SageMaker kernel**: conda_pytorch_p36\n",
    "\n",
    "## Process description\n",
    "\n",
    "The continuous stirred tank reactor (CSTR) is a common model for a chemical reactor in chemical engineering that assumes perfect mixing. Therefore, the output composition is identical to the composition of the material inside the reactor, which is a function of the residence time and the reaction rate.\n",
    "\n",
    "<img src=\"continuous-stirred-tank-reactor.png\" alt=\"CSTR drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "The process has four states: concentration of reactant A ($C_A$), concentration of reactant B ($C_B$), temperature in the reactor ($T_R$), and temperature in the cooling jacket ($T_K$). There are two inputs (set points) in the system: feed flow rate ($F$) and heat flux ($\\dot{Q}$). This reactor receives reactant A and releases a solution that contains both reactant A and B with concentrations $C_A$ and $C_B$.\n",
    "\n",
    "The objective for a process engineer is to find **optimal set points** ($F$ and $\\dot{Q}$) that will produce a mixture with **desired concentration** of B $C_B$, while **preserving safe operating conditions** for the cooling jacket, i.e., 50 [C] $\\le T_K \\le$ 140 [C]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd05e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "# Insert the name of your S3 bucket from CloudFormation outputs\n",
    "bucket = \"\" \n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = \"us-east-1\"\n",
    "prefix = \"models/pytorch\"\n",
    "\n",
    "assert bucket, \"ERROR: Insert the name of your S3 bucket before moving on\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6a706",
   "metadata": {},
   "source": [
    "## Historical data set\n",
    "\n",
    "You already have historical data from an industrial data lake (similar to the one you built in the first part of the workshop). Measurements have been consolidated into a single csv file for easy consumption. The data set is loaded into a `pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dde79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "FILE_PATH = os.path.join(os.getcwd(), \"data\", \"historical\", \"measurements.csv\")\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761af39",
   "metadata": {},
   "source": [
    "The first step will be the exploration of the data set in search for insight into the process dynamics. From these plots we can observe that:\n",
    "\n",
    "- When $F$ increases $C_A$ increases\n",
    "- Temperatures $T_R$ and $T_K$ are positively correlated\n",
    "- $C_B$ increases when $C_A$ increases until it plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfe8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.plotting.scatter_matrix(df, figsize=(12,12), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ce222",
   "metadata": {},
   "source": [
    "## Building a process model\n",
    "\n",
    "Historical data will now be used to build a machine learning model of the reactor dynamics. The following cell shows the script of that defined a process model designed in PyTorch, an open source machine learning framework. The model will take two inputs (`F` and `Q_dot`) and will return three outputs (`C_a`, `C_b`, and `T_K`).\n",
    "\n",
    "Spend a minute exploring the structure of the multi layer perceptron model that is used to predict the behavior of the CSTR process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6b6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize CSTRModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70ae28",
   "metadata": {},
   "source": [
    "Before training the model, historical data will be split into a training and a validation set, which will have 90% and 10% of the data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2694d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_data = df[[\"C_a\", \"C_b\", \"T_K\", \"F\", \"Q_dot\"]]\n",
    "train_data, validation_data = np.split(\n",
    "    model_data.sample(frac=1, random_state=42),\n",
    "    [int(0.9 * len(model_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8b14a",
   "metadata": {},
   "source": [
    "The `process_dataframe` function scales input and output variables before training the `CSTRModel` to improve training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc91ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess a pandas dataframe to extract\n",
    "    independent and depent variables, and scale them\n",
    "    \"\"\"\n",
    "    X = df[['F', 'Q_dot']].values\n",
    "    Y = df[['C_a', 'C_b', 'T_K']].values\n",
    "    # Scale F between 5 and 100\n",
    "    X[:,0] = (X[:,0]-5.0)/95.0\n",
    "    # Scale Q_dot between -5000 and 0\n",
    "    X[:,1] = (X[:,1]+5000.0)/5000.0\n",
    "    # Scale T_K between 125.0 and 150.0\n",
    "    Y[:,2] = (Y[:,2]-125.0)/25.0\n",
    "    # Convert to PyTorch tensors\n",
    "    X = torch.from_numpy(X.astype(np.float32))\n",
    "    Y = torch.from_numpy(Y.astype(np.float32))\n",
    "    \n",
    "    return (X, Y)\n",
    "\n",
    "X_train, Y_train = process_dataframe(train_data)\n",
    "X_validation, Y_validation = process_dataframe(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8004c308",
   "metadata": {},
   "source": [
    "We will now select a learning rate, a loss function, and an optimizer. In this case we will use the Adam optimizer with a learning rate of 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b57414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CSTRModel import CSTRModel\n",
    "import torch.nn as nn\n",
    "\n",
    "model = CSTRModel()\n",
    "learning_rate = 0.0005\n",
    "l = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr =learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ab8803",
   "metadata": {},
   "source": [
    "Now we train the model. The model is small and it should only take 1 to 2 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "\n",
    "num_epochs = 1000\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward feed\n",
    "    Y_pred = model(X_train.requires_grad_())\n",
    "    # Calculate the training loss\n",
    "    t_loss = l(Y_pred, Y_train)\n",
    "    training_loss.append(t_loss.item())\n",
    "    # Calculate validation loss\n",
    "    Y_pred = model(X_validation)\n",
    "    v_loss = l(Y_pred, Y_validation)\n",
    "    validation_loss.append(v_loss.item())\n",
    "    # Backpropagation: calculate gradients\n",
    "    t_loss.backward()\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    # Clear out the gradients from the last step loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "\n",
    "print(\"Model training took {:.1f} seconds\".format(toc-tic))\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(training_loss, color=\"steelblue\", label=\"training\")\n",
    "plt.plot(validation_loss, color=\"firebrick\", label=\"validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\"); plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c8fa6",
   "metadata": {},
   "source": [
    "### Evaluate process model\n",
    "\n",
    "We now can evaluate the `CSTRModel` with various conditions (i.e., different combinations of `F` and `Q_dot`) and compare them to historical data. One quick evaluation we can conduct is to evaluate the model for the same condition simulated with the `steady_state_cstr` model, i.e., $F$ = 12.0 [l/h] and $\\dot{Q}$ = -10.0 [kW]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59609558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pytorch_model(model, manipulated_vars):\n",
    "    \"\"\"\n",
    "    Return prediction for a set of manipulated variables\n",
    "    \"\"\"\n",
    "    X_test = manipulated_vars[[\"F\", \"Q_dot\"]].values\n",
    "    X_test[:,0] = (X_test[:,0]-5.0)/95.0\n",
    "    X_test[:,1] = (X_test[:,1]+5000.0)/5000.0\n",
    "    X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "    Y_test = model(X_test.float()).cpu().detach().numpy()\n",
    "    Y_test[:,2] = (Y_test[:,2]*25.0)+125.0\n",
    "    prediction = pd.DataFrame({\"F\" : manipulated_vars[\"F\"].values.round(2),\n",
    "                              \"Q_dot\": manipulated_vars[\"Q_dot\"].values.round(1),\n",
    "                              \"C_a\": Y_test[:,0].round(4),\n",
    "                              \"C_b\": Y_test[:,1].round(4),\n",
    "                              \"T_K\": Y_test[:,2].round(1)})\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulated_vars = pd.DataFrame({\"F\": [12.0], \"Q_dot\": [-10.0]})\n",
    "\n",
    "prediction = evaluate_pytorch_model(model, manipulated_vars)\n",
    "measurements = prediction.loc[0,:]\n",
    "\n",
    "print(\"Measurements from CSTR:\")\n",
    "print(\"Flow rate: {:.2f} l/h\".format(measurements[\"F\"]))\n",
    "print(\"Concentration of reactant A: {:.4f} mol A/l\".format(measurements[\"C_a\"]))\n",
    "print(\"Concentration of reactant B: {:.4f} mol B/l\".format(measurements[\"C_b\"]))\n",
    "print(\"Temperature in the cooling jacket: {:.2f} C\".format(measurements[\"T_K\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef6a37",
   "metadata": {},
   "source": [
    "Now we will persist the model so it can be reused without having to retrain it. The model will be put inside a TGZ file and uploaded to an S3 bucket so we can deploy it to the edge device later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Trace the whole module (class) and construct a ScriptModule with a single forward method\n",
    "module = torch.jit.trace(model.float().eval(), torch.rand(1,2).float())\n",
    "module.save(\"cstr.pth\")\n",
    "model_name = \"cstr-model\"\n",
    "\n",
    "try:\n",
    "    os.remove(\"{:s}.tar.gz\".format(model_name))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "with tarfile.open(\"{:s}.tar.gz\".format(model_name), \"w:gz\") as f:\n",
    "    f.add(\"cstr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40327d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "model_path = sess.upload_data(path=\"{:s}.tar.gz\".format(model_name), bucket = bucket, key_prefix=prefix)\n",
    "model_s3_uri = \"s3://{:s}/{:s}/{:s}.tar.gz\".format(bucket, prefix, model_name)\n",
    "print(\"The model tarball is available at: \", model_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec7726",
   "metadata": {},
   "source": [
    "## Process maps\n",
    "\n",
    "One approach to explore the behavior of a forward model is through a **process map**, which leverages our model to predict controlled variables for different combinations of manipulated variables. We will construct a grid of combinations of manipulated variables and use our model to predict the expected behavior of the CSTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Construct grid\n",
    "n_x, n_y = 100, 100\n",
    "F_1d = np.linspace(5.0, 100.0, n_x)\n",
    "Q_dot_1d = np.linspace(-5000.0, 0.0, n_y)\n",
    "\n",
    "F_2d, Q_dot_2d = np.meshgrid(F_1d, Q_dot_1d)\n",
    "\n",
    "map_df = pd.DataFrame({\"F\": F_2d.flatten(),\n",
    "                      \"Q_dot\": Q_dot_2d.flatten()})\n",
    "map_pred = evaluate_pytorch_model(model, map_df)\n",
    "\n",
    "C_a_2d = map_pred[\"C_a\"].values.reshape(n_x, n_y)\n",
    "C_b_2d = map_pred[\"C_b\"].values.reshape(n_x, n_y)\n",
    "T_K_2d = map_pred[\"T_K\"].values.reshape(n_x, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1e5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(figsize=(22, 5))\n",
    "# Plot concentration of reactant A [mol A/l]\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "im1 = ax1.contourf(F_2d, Q_dot_2d, C_a_2d)\n",
    "divider = make_axes_locatable(ax1)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "ax1.set_xlabel(\"Feed flow rate [l/h]\")\n",
    "ax1.set_ylabel(\"Heat flow [kW]\")\n",
    "ax1.title.set_text(\"Exit flow of reactant A [mol A/l]\")\n",
    "\n",
    "# Plot concentration of reactant B [mol B/l]\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "im2 = ax2.contourf(F_2d, Q_dot_2d, C_b_2d)\n",
    "divider = make_axes_locatable(ax2)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im2, cax=cax, orientation='vertical')\n",
    "ax2.set_xlabel(\"Feed flow rate [l/h]\")\n",
    "ax2.set_ylabel(\"Heat flow [kW]\")\n",
    "ax2.title.set_text(\"Exit flow of reactant B [mol B/l]\")\n",
    "\n",
    "# Plot reactor temperature [C]\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "im3 = ax3.contourf(F_2d, Q_dot_2d, T_K_2d)\n",
    "divider = make_axes_locatable(ax3)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im3, cax=cax, orientation='vertical')\n",
    "ax3.set_xlabel(\"Feed flow rate [l/h]\")\n",
    "ax3.set_ylabel(\"Heat flow [kW]\")\n",
    "ax3.title.set_text(\"Cooling jacket temperature [C]\")\n",
    "\n",
    "fig.suptitle(\"Process map\", fontsize=20, y=0.95)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecae7b6",
   "metadata": {},
   "source": [
    "## Process optimization\n",
    "\n",
    "The first use of the process model `model` will be to help with process optimization by selecting the inputs that are likely to result in desired behavior. Let us consider the case when the process engineer would like to achieve a desired $C_B^{*}$ at the output of the CSTR, while satisfying safety condition (soft constraints)\n",
    "\n",
    "$$\n",
    "50.0 [C] \\le T_K \\le 140.0 [C]\n",
    "$$\n",
    "\n",
    "and the admissible ranges for the manipulated variables (hard constraints)\n",
    "\n",
    "$$\n",
    "5 [l/h] \\le F \\le 100.0 [l/h] \\\\\n",
    "-5000.0 [kW] \\le \\dot{Q} \\le 0.0 [kW]\n",
    "$$\n",
    "\n",
    "In this example we will use the `dual_annealing` function for `scipy` to find optimal manipulated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import dual_annealing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "def optimal_manipulated_vars(C_b_ref):\n",
    "    \"\"\"\n",
    "    Drive CSTR toward C_b_ref\n",
    "    \"\"\"\n",
    "    C_b_ref = C_b_ref\n",
    "    # Admissible bounds for manipulated variables\n",
    "    bounds = ((5.0, 100.0), (-5000.0, 0.0))\n",
    "    \n",
    "    def objective(x):\n",
    "        '''\n",
    "        Objective function to minimize: Penalizes deviations from\n",
    "        C_b_ref and T_K leaving the desired range\n",
    "        '''\n",
    "        manipulated_vars = pd.DataFrame({\"F\": [x[0]], \"Q_dot\" : [x[1]]})\n",
    "        prediction = evaluate_pytorch_model(model, manipulated_vars)\n",
    "        cost = 0.0\n",
    "        # Deviation from reference C_b_ref\n",
    "        cost += (prediction[\"C_b\"][0]-C_b_ref)**2\n",
    "        # Cooling jacket temperature\n",
    "        if (prediction[\"T_K\"][0] < 5.0):\n",
    "            cost += 0.01*(prediction[\"T_K\"][0]-5.0)**2\n",
    "        elif (prediction[\"T_K\"][0] > 140.0):\n",
    "            cost += 0.01*(prediction[\"T_K\"][0]-140.0)**2\n",
    "\n",
    "        return cost\n",
    "    tic=timeit.default_timer()\n",
    "    result = dual_annealing(objective, bounds=bounds, maxiter=2000)\n",
    "    toc=timeit.default_timer()\n",
    "    \n",
    "    return (result['x'], result['nfev'], toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711e5a0",
   "metadata": {},
   "source": [
    "We can evaluate the optimization function and calculate the optimal manipulated variables to reach $C_B$ = 0.6 [mol/l], while staying within the bounds for flow rate, heat rate, and cooling jacket temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# Find optimal conditions to drive CSTR to 0.6 [mol/l]\n",
    "x, nfev, elapsed_time = optimal_manipulated_vars(0.6)\n",
    "\n",
    "# Summarize the result\n",
    "print('Total Evaluations: {:d}'.format(nfev))\n",
    "# Evaluate solution\n",
    "print(\"The optimal feed flow rate is {:.1f} [l/h]\".format(x[0]))\n",
    "print(\"The optimal heat flux is {:.0f} [kW]\".format(x[1]))\n",
    "controlled_vars = evaluate_pytorch_model(model, pd.DataFrame({\"F\": [x[0]], \"Q_dot\": [x[1]]}))\n",
    "print(\"The predicted concentration of B is {:.4f} [mol/l]\".format(controlled_vars[\"C_b\"][0]))\n",
    "print(\"The cooling jacket temperature was {:.1f} [C]\".format(controlled_vars[\"T_K\"][0]))\n",
    "print(\"Optimization took {:.1f} seconds\".format(elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
